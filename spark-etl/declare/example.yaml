sources:
  - type: csv
    name: source1
    options:
      paths: [ "/tmp/test.csv" ]                          # 必填
      header: true                                        # 非必填，默认 false
      encoding: "UTF-8"                                   # 非必填，默认 utf-8
      nullValue:                                          # 非必填，默认 null
      nanValue: "NaN"                                     # 非必填，默认 NaN
      dateFormat: "yyyy-MM-dd"                            # 非必填，默认 yyyy-MM-dd
      dateTimeFormat: "yyyy-MM-dd HH:mm:ss.SSS"           # 非必填，默认 yyyy-MM-dd HH:mm:ss.SSS
      parseErrorPolicy: "PERMISSIVE"                      # 非必填，默认 PERMISSIVE，可选值有【PERMISSIVE，DROPMAL_FORMED，FAILFAST】
      recursiveFileLookup: true                           # 非必填，默认 true
    output: df_1
    repartition: 20

transforms:
  - type: dataType
    options:
      - type: "StringToDate"                              # 判断使用哪种数据类型
        params:
          convert-columns: [ "col_1" ]                     # 需要类型转换的字段
          column-alias:
            col_1: col1
          dataFormat: "yyyy-MM-dd"
      - type: "IntToString"
        params:
          convert-columns: [ "col_2" ]
          column-alias:
            col_2: col2
    outputs: df_11
    repartition: 20

  - type: sparkSql
    options:
      params:
        c1: 1
      sql: select * from t11 where col1 > ${c1}
    output: "df_111"
    repartition: 20                                          # 非必填

sinks:
  - type: csv
    input: df_111
    options:
      paths: [/tmp/csv_out]
      header: false              # 非必填，默认 true
      writeMode: overwrite