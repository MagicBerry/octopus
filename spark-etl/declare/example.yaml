sources:
  - type: csv
    name: source1
    options:
      paths: [ "/tmp/test.csv" ]                          # 必填
      header: true                                        # 非必填，默认 false
      encoding: "UTF-8"                                   # 非必填，默认 utf-8
      nullValue:                                          # 非必填，默认 null
      nanValue: "NaN"                                     # 非必填，默认 NaN
      dateFormat: "yyyy-MM-dd"                            # 非必填，默认 yyyy-MM-dd
      dateTimeFormat: "yyyy-MM-dd HH:mm:ss.SSS"           # 非必填，默认 yyyy-MM-dd HH:mm:ss.SSS
      parseErrorPolicy: "PERMISSIVE"                      # 非必填，默认 PERMISSIVE，可选值有【PERMISSIVE，DROPMAL_FORMED，FAILFAST】
      recursiveFileLookup: true                           # 非必填，默认 true
    outputs:
      - alias: df_1
        columns: [ "col_1", "col_2" ]

transforms:
  - type: dataType
    inputs:
      df_1: t1
    options:
      - type: "StringToDate"                              # 判断使用哪种数据类型
        params:
          convert-columns: [ "col_1" ]                     # 需要类型转换的字段
          column-alias:
            col_1: col1
          dataFormat: "yyyy-MM-dd"
      - type: "IntToString"
        params:
          convert-columns: [ "col_2" ]
          column-alias:
            col_2: col2
    outputs: [ df_11 ]

  - type: sparkSql
    inputs:
      df_11: t11
    options:
      params:
        c1: 1
      sql: select * from t11 where col1 > ${c1}
    outputs: [ "df_111" ]
    repartition:
      nums: 20                           # 非必填
      exprs:

sinks:
  - type: csv
    input: [df_111]
    options:
      paths: [/tmp/csv_out]
      header: false              # 非必填，默认 true
      writeMode: overwrite